{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Old Tweets of the #eredivisie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you run this document with 'Cell -> Run All', the document will scrape the following tweets to a CSV:\n",
    "<br>\n",
    "<b>Explanation:</b> <p>\n",
    "We will scrape tweets from twitter.com with hashtags of the 18 eredivisie clubs to see if the conversation on twitter changed about football clubs in the Netherlands due to COVID-19<p>\n",
    "<br>\n",
    "<b>Dates:</b> <p>\n",
    "<i>Season 19/20</i><p>\n",
    "Period 1: Round 14 - 22 23 24 november 2019 <p>\n",
    "Period 2: Round 20 - 24 25 26 januari 2020<p>\n",
    "<br>\n",
    "<i>Season 20/21</i><p>\n",
    "Period 1: Round 10 - 27 28 29 november 2020<p>\n",
    "Period 2: Round 18 - 22 23 24 januari 2021 <p>\n",
    "<br>\n",
    "<b>Hashtags:</b><p>\n",
    "#AdoDenHaag\n",
    "#AFCAjax\n",
    "#AZalkmaar\n",
    "#FCEmmen\n",
    "#FCGroningen\n",
    "#FCTwente\n",
    "#FCUtrecht\n",
    "#Feyenoord\n",
    "#FortunaSittard\n",
    "#Heracles\n",
    "#PECZwolle\n",
    "#PSV\n",
    "#RKCWaalwijk\n",
    "#SCHeerenveen\n",
    "#SpartaRotterdam\n",
    "#Vitesse\n",
    "#VVVVenlo\n",
    "#WillemII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now running some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MartinBeckUT/TwitterScraper/blob/master/snscrape/python-wrapper/snscrape-python-wrapper.ipynb\n",
    "\n",
    "# Run the pip install command below if you don't already have the library\n",
    "\n",
    "# Installs\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "!pip install snscrape\n",
    "!pip install pandas\n",
    "!pip install dropbox\n",
    "!pip install pathlib\n",
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import pathlib\n",
    "import dropbox\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query by Hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping\n",
    "\n",
    "def get_twitterdata(filename, \n",
    "                    season,\n",
    "                    period,\n",
    "                    maxTweets = 100,\n",
    "                    searchq = ['#afcajax'],\n",
    "                    since = '2021-01-01', until='2021-01-03'):\n",
    "    \n",
    "    # Begin with X tweets to search in order to see if it works without taking too much time\n",
    "    timefilter= 'since:' + since +' until:'+until\n",
    "    print(\"Hi! I'm starting up!\")\n",
    "    print('for time period: '+ timefilter)\n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list1 = [] \n",
    "\n",
    "    # Using SNScraper to scrape data and append tweets to list\n",
    "\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(searchq + ' ' + timefilter).get_items()):\n",
    "        if i>maxTweets: #stops when amount of tweets is at its maximum\n",
    "            break\n",
    "            \n",
    "        tweets_list1.append([season, period, tweet.url, tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.user.location, tweet.user.followersCount, tweet.user.friendsCount, tweet.source]) #add all the items we want to subtrect\n",
    "    print('Done scraping! Saving now :)')\n",
    "    \n",
    "    # Creating a dataframe from the tweets list above\n",
    "    df_tweets1 = pd.DataFrame(tweets_list1, columns=['Season', 'Period','URL', 'Datetime', 'Tweet Id', 'Text', 'Username', 'Replies', 'Retweets', 'Likes', 'Location', 'Followers', 'Friends', 'Source']) #give the collumns names and create our Data Frame\n",
    "    df_tweets1.to_csv(path + filename, sep=',', index=False)\n",
    "    \n",
    "    print(f'Done with data from {season} {period}!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing hashtags\n",
    "eredivisie_clubs= '#AdoDenHaag OR #AFCAjax OR #AZalkmaar OR #FCEmmen OR #FCGroningen OR #FCTwente OR #FCUtrecht OR #Feyenoord OR #FortunaSittard OR #Heracles OR #PECZwolle OR #PSV OR #RKCWaalwijk OR #SCHeerenveen OR #SpartaRotterdam OR #Vitesse OR #VVVVenlo OR #WillemII'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now scraping tweets for Eredivisie Hashtags, Season X and Period X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Season 19/20, period 1\n",
    "get_twitterdata(filename= 'season19-20period1max100.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2019-11-22',\n",
    "                until = '2019-11-24',\n",
    "                season= 'season19/20',\n",
    "                period= 'period1')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 19/20, period 2\n",
    "get_twitterdata(filename= 'season19-20period2max100.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2020-01-24',\n",
    "                until = '2020-01-26',\n",
    "                season= 'season19/20',\n",
    "                period= 'period2')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 20/21, period 1\n",
    "get_twitterdata(filename= 'season20-21period1max100.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2020-11-27',\n",
    "                until = '2020-11-29',\n",
    "                season= 'season20/21',\n",
    "                period= 'period1')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 20/21, period 2\n",
    "get_twitterdata(filename= 'season20-21period2max100.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2021-01-22',\n",
    "                until = '2021-01-24',\n",
    "                season= 'season20/21',\n",
    "                period= 'period2')\n",
    "\n",
    "print(\"all done! :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now merging and exporting local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find_All_CSVs = ['season19-20period1max100.csv',\n",
    "                 'season19-20period2max100.csv',\n",
    "                 'season20-21period1max100.csv',\n",
    "                 'season20-21period2max100.csv']\n",
    "\n",
    "#combine all files in the list\n",
    "\n",
    "combined_csv = pd.concat([pd.read_csv(f,header=0) for f in Find_All_CSVs])\n",
    "\n",
    "combined_csv.head()\n",
    "\n",
    "combined_csv.to_csv(\"..\\..\\oDCM\\data\\merged_tweets_eredivisiemax100.csv, quotechar='\"',\n",
    "          quoting=csv.QUOTE_ALL, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"all done! twitter data available in your directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now exporting to Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferData:\n",
    "    def __init__(self, access_token):\n",
    "        self.access_token = access_token\n",
    "\n",
    "    def upload_file(self, file_from, file_to):\n",
    "        \"\"\"upload a file to Dropbox using API v2\n",
    "        \"\"\"\n",
    "        dbx = dropbox.Dropbox(self.access_token)\n",
    "\n",
    "        with open(file_from, 'rb') as f:\n",
    "            dbx.files_upload(f.read(), file_to)\n",
    "\n",
    "def main():\n",
    "    access_token = 'Dropbox-Token'\n",
    "    transferData = TransferData(access_token)\n",
    "\n",
    "    file_from = 'merged_tweets_eredivisie.csv'\n",
    "    file_to = '/Apps/dPrep/merged_tweets_eredivisie.csv'  # The full path to upload the file to, including the file name\n",
    "\n",
    "    # API v2\n",
    "    transferData.upload_file(file_from, file_to)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "print(\"All done, check out the dropbox folder: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
