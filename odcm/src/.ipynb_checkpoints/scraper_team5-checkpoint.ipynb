{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping old Tweets of the #eredivisie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# ------------ WARNING ----------------\n",
    "\n",
    "<br>\n",
    "<b>Scraper does not work with python versions below 3.8.5!</b><p>\n",
    "Please download te latest version of python before using this scraper and put the scraper into the Path of your computer<p>\n",
    "<b>Python download:</b> https://www.python.org/downloads/<p>\n",
    "<b>Path explanation:</b> https://tilburgsciencehub.com/building-blocks/configure-your-computer/statistics-and-computation/r/<p>\n",
    "                  https://tilburgsciencehub.com/building-blocks/configure-your-computer/automation-and-workflows/environment/<p>\n",
    "\n",
    "# ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you run this document with 'Cell -> Run All', the document will scrape the following tweets to a CSV:\n",
    "<br>\n",
    "<b>Explanation:</b> <p>\n",
    "We will scrape tweets from Twitter with hashtags of the 18 eredivisie clubs to see if the conversation on Twitter changed about football clubs in the Netherlands due to COVID-19<p>\n",
    "<br>\n",
    "<b>Dates:</b> <p>\n",
    "<i>Season 19/20</i><p>\n",
    "Period 1: Round 14 - 22 23 24 november 2019 <p>\n",
    "Period 2: Round 20 - 24 25 26 januari 2020<p>\n",
    "<br>\n",
    "<i>Season 20/21</i><p>\n",
    "Period 1: Round 10 - 27 28 29 november 2020<p>\n",
    "Period 2: Round 18 - 22 23 24 januari 2021 <p>\n",
    "<br>\n",
    "<b>Hashtags:</b><p>\n",
    "#AdoDenHaag\n",
    "#AFCAjax\n",
    "#AZalkmaar\n",
    "#FCEmmen\n",
    "#FCGroningen\n",
    "#FCTwente\n",
    "#FCUtrecht\n",
    "#Feyenoord\n",
    "#FortunaSittard\n",
    "#Heracles\n",
    "#PECZwolle\n",
    "#PSV\n",
    "#RKCWaalwijk\n",
    "#SCHeerenveen\n",
    "#SpartaRotterdam\n",
    "#Vitesse\n",
    "#VVVVenlo\n",
    "#WillemII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now running some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /private/var/folders/ww/4ympmb955lv8kww_51z1cn1m0000gn/T/pip-req-build-o2yahch3\n",
      "Requirement already satisfied (use --upgrade to upgrade): snscrape==0.3.5.dev96+g47fbc2a from git+https://github.com/JustAnotherArchivist/snscrape.git in /opt/anaconda3/lib/python3.8/site-packages\n",
      "Requirement already satisfied: requests[socks] in /opt/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2.24.0)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.6.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.9.3)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2020.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->snscrape==0.3.5.dev96+g47fbc2a) (2.0.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for snscrape: filename=snscrape-0.3.5.dev96+g47fbc2a-py3-none-any.whl size=50124 sha256=5c9585de32b10ccd375217e60a1b03a82dcfb55172e9372922f32ed83ad689a8\n",
      "  Stored in directory: /private/var/folders/ww/4ympmb955lv8kww_51z1cn1m0000gn/T/pip-ephem-wheel-cache-l7v3jgh_/wheels/92/42/87/33fa9b18f7a75d02643a9ca3743339aec9be28c6796267c7d8\n",
      "Successfully built snscrape\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: dropbox in /opt/anaconda3/lib/python3.8/site-packages (11.4.1)\n",
      "Requirement already satisfied: requests>=2.16.2 in /opt/anaconda3/lib/python3.8/site-packages (from dropbox) (2.24.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.8/site-packages (from dropbox) (1.15.0)\n",
      "Requirement already satisfied: stone>=2.* in /opt/anaconda3/lib/python3.8/site-packages (from dropbox) (3.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.16.2->dropbox) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.16.2->dropbox) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.16.2->dropbox) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.16.2->dropbox) (1.25.11)\n",
      "Requirement already satisfied: ply>=3.4 in /opt/anaconda3/lib/python3.8/site-packages (from stone>=2.*->dropbox) (3.11)\n",
      "Requirement already satisfied: pathlib in /opt/anaconda3/lib/python3.8/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Run the pip install command below if you don't already have the libraries\n",
    "\n",
    "# Installs\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "!pip install pandas\n",
    "!pip install dropbox\n",
    "!pip install pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import dropbox\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping\n",
    "\n",
    "def get_twitterdata(filename, \n",
    "                    season,\n",
    "                    period,\n",
    "                    path=\"\",\n",
    "                    maxTweets = 10000,\n",
    "                    searchq = ['#afcajax'],\n",
    "                    since = '2021-01-01', until='2021-01-03'):\n",
    "    \n",
    "    # Begin with X tweets to search in order to see if it works without taking too much time\n",
    "    timefilter= 'since:' + since +' until:'+until\n",
    "    print(\"Hi! I'm starting up!\")\n",
    "    print('for time period: '+ timefilter)\n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list1 = [] \n",
    "\n",
    "    # Using snscrape to scrape data and append tweets to list\n",
    "\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(searchq + ' ' + timefilter).get_items()):\n",
    "        if i>maxTweets: #stops when amount of tweets is at its maximum\n",
    "            break\n",
    "            \n",
    "        tweets_list1.append([season, period, tweet.url, tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.user.location, tweet.user.followersCount, tweet.user.friendsCount, tweet.source]) #add all the items we want to subtrect\n",
    "    print('Done scraping! Saving now :)')\n",
    "    \n",
    "    # Creating a dataframe from the tweets list above\n",
    "    df_tweets1 = pd.DataFrame(tweets_list1, columns=['Season', 'Period','URL', 'Datetime', 'Tweet Id', 'Text', 'Username', 'Replies', 'Retweets', 'Likes', 'Location', 'Followers', 'Friends', 'Source']) #give the collumns names and create our Data Frame\n",
    "    df_tweets1.to_csv(path + filename, sep=',', index=False)\n",
    "    \n",
    "    print(f'Done with data from {season} {period}!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing hashtags in variable \n",
    "eredivisie_clubs= '#AdoDenHaag OR #AFCAjax OR #AZalkmaar OR #FCEmmen OR #FCGroningen OR #FCTwente OR #FCUtrecht OR #Feyenoord OR #FortunaSittard OR #Heracles OR #PECZwolle OR #PSV OR #RKCWaalwijk OR #SCHeerenveen OR #SpartaRotterdam OR #Vitesse OR #VVVVenlo OR #WillemII'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now scraping tweets for Eredivisie Hashtags, Season X and Period X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm starting up!\n",
      "for time period: since:2019-11-22 until:2019-11-24\n",
      "Done scraping! Saving now :)\n",
      "Done with data from season19/20 period1!\n",
      "Hi! I'm starting up!\n",
      "for time period: since:2020-01-24 until:2020-01-26\n",
      "Done scraping! Saving now :)\n",
      "Done with data from season19/20 period2!\n",
      "Hi! I'm starting up!\n",
      "for time period: since:2020-11-27 until:2020-11-29\n",
      "Done scraping! Saving now :)\n",
      "Done with data from season20/21 period1!\n",
      "Hi! I'm starting up!\n",
      "for time period: since:2021-01-22 until:2021-01-24\n"
     ]
    }
   ],
   "source": [
    "# Season 19/20, period 1\n",
    "get_twitterdata(filename= 'season19-20period1.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2019-11-22',\n",
    "                until = '2019-11-24',\n",
    "                season= 'season19/20',\n",
    "                period= 'period1'#,Uncomment next line to test scraper for only 100 tweets!\n",
    "                #maxTweets = 100\n",
    "               )\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 19/20, period 2\n",
    "get_twitterdata(filename= 'season19-20period2.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2020-01-24',\n",
    "                until = '2020-01-26',\n",
    "                season= 'season19/20',\n",
    "                period= 'period2'#,Uncomment next line to test scraper for only 100 tweets!\n",
    "                #maxTweets = 100\n",
    "               )\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 20/21, period 1\n",
    "get_twitterdata(filename= 'season20-21period1.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2020-11-27',\n",
    "                until = '2020-11-29',\n",
    "                season= 'season20/21',\n",
    "                period= 'period1'#,Uncomment next line to test scraper for only 100 tweets!\n",
    "                #maxTweets = 100\n",
    "               )\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 20/21, period 2\n",
    "get_twitterdata(filename= 'season20-21period2.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2021-01-22',\n",
    "                until = '2021-01-24',\n",
    "                season= 'season20/21',\n",
    "                period= 'period2'#,Uncomment next line to test scraper for only 100 tweets!\n",
    "                #maxTweets = 100\n",
    "               )\n",
    "\n",
    "print(\"All done! :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now merging and exporting local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find_All_CSVs = ['../../oDCM/data/season19-20period1.csv',\n",
    "                 '../../oDCM/data/season19-20period2.csv',\n",
    "                 '../../oDCM/data/season20-21period1.csv',\n",
    "                 '../../oDCM/data/season20-21period2.csv']\n",
    "\n",
    "# Combine all rows of CSV-files in the list above to one 'merged_tweets_eredivisie.csv' file.\n",
    "\n",
    "combined_csv = pd.concat([pd.read_csv(f,header=0) for f in Find_All_CSVs])\n",
    "\n",
    "combined_csv.head()\n",
    "\n",
    "combined_csv.to_csv(\"../../oDCM/data/merged_tweets_eredivisie.csv\", quotechar='\"', quoting=csv.QUOTE_ALL, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"All done! Twitter data available in your directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now exporting to Dropbox (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment full codeblock below (select all, use shortcut *ctr* + */* or *cmd* + */*) and add own Dropbox-token to directly upload scraped dataset to own dropbox folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransferData:\n",
    "#     def __init__(self, access_token):\n",
    "#         self.access_token = access_token\n",
    "\n",
    "#     def upload_file(self, file_from, file_to):\n",
    "#         \"\"\"upload a file to Dropbox using API v2\n",
    "#         \"\"\"\n",
    "#         dbx = dropbox.Dropbox(self.access_token)\n",
    "\n",
    "#         with open(file_from, 'rb') as f:\n",
    "#             dbx.files_upload(f.read(), file_to)\n",
    "\n",
    "# def main():\n",
    "#     access_token = 'Dropbox-Token'\n",
    "#     transferData = TransferData(access_token)\n",
    "\n",
    "#     file_from = 'merged_tweets_eredivisie.csv'\n",
    "#     file_to = '/Apps/dPrep/merged_tweets_eredivisie.csv'  # The full path to upload the file to, including the file name\n",
    "\n",
    "#     # API v2\n",
    "#     transferData.upload_file(file_from, file_to)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "    \n",
    "# print(\"All done, check out the dropbox folder: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
