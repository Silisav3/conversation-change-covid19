{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping old Tweets of the #eredivisie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# ------------ WARNING ----------------\n",
    "\n",
    "<br>\n",
    "<b>Scraper does not work with python versions below 3.8.5!</b><p>\n",
    "Please download te latest version of python before using this scraper and put the scraper into the Path of your computer<p>\n",
    "<b>Python download:</b> https://www.python.org/downloads/<p>\n",
    "<b>Path explanation:</b> https://tilburgsciencehub.com/building-blocks/configure-your-computer/statistics-and-computation/r/<p>\n",
    "                  https://tilburgsciencehub.com/building-blocks/configure-your-computer/automation-and-workflows/environment/<p>\n",
    "\n",
    "# ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you run this document with 'Cell -> Run All', the document will scrape the following tweets to a CSV:\n",
    "<br>\n",
    "<b>Explanation:</b> <p>\n",
    "We will scrape tweets from Twitter with hashtags of the 18 eredivisie clubs to see if the conversation on Twitter changed about football clubs in the Netherlands due to COVID-19<p>\n",
    "<br>\n",
    "<b>Dates:</b> <p>\n",
    "<i>Season 19/20</i><p>\n",
    "Period 1: Round 14 - 22 23 24 november 2019 <p>\n",
    "Period 2: Round 20 - 24 25 26 januari 2020<p>\n",
    "<br>\n",
    "<i>Season 20/21</i><p>\n",
    "Period 1: Round 10 - 27 28 29 november 2020<p>\n",
    "Period 2: Round 18 - 22 23 24 januari 2021 <p>\n",
    "<br>\n",
    "<b>Hashtags:</b><p>\n",
    "#AdoDenHaag\n",
    "#AFCAjax\n",
    "#AZalkmaar\n",
    "#FCEmmen\n",
    "#FCGroningen\n",
    "#FCTwente\n",
    "#FCUtrecht\n",
    "#Feyenoord\n",
    "#FortunaSittard\n",
    "#Heracles\n",
    "#PECZwolle\n",
    "#PSV\n",
    "#RKCWaalwijk\n",
    "#SCHeerenveen\n",
    "#SpartaRotterdam\n",
    "#Vitesse\n",
    "#VVVVenlo\n",
    "#WillemII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now running some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pip install command below if you don't already have the libraries\n",
    "\n",
    "# Installs\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "!pip install pandas\n",
    "!pip install dropbox\n",
    "!pip install pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import dropbox\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping\n",
    "\n",
    "def get_twitterdata(filename, \n",
    "                    season,\n",
    "                    period,\n",
    "                    path=\"\",\n",
    "                    maxTweets = 10000,\n",
    "                    searchq = ['#afcajax'],\n",
    "                    since = '2021-01-01', until='2021-01-03'):\n",
    "    \n",
    "    # Begin with X tweets to search in order to see if it works without taking too much time\n",
    "    timefilter= 'since:' + since +' until:'+until\n",
    "    print(\"Hi! I'm starting up!\")\n",
    "    print('for time period: '+ timefilter)\n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list1 = [] \n",
    "\n",
    "    # Using snscrape to scrape data and append tweets to list\n",
    "\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(searchq + ' ' + timefilter).get_items()):\n",
    "        if i>maxTweets: #stops when amount of tweets is at its maximum\n",
    "            break\n",
    "            \n",
    "        tweets_list1.append([season, period, tweet.url, tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.user.location, tweet.user.followersCount, tweet.user.friendsCount, tweet.source]) #add all the items we want to subtrect\n",
    "    print('Done scraping! Saving now :)')\n",
    "    \n",
    "    # Creating a dataframe from the tweets list above\n",
    "    df_tweets1 = pd.DataFrame(tweets_list1, columns=['Season', 'Period','URL', 'Datetime', 'Tweet Id', 'Text', 'Username', 'Replies', 'Retweets', 'Likes', 'Location', 'Followers', 'Friends', 'Source']) #give the collumns names and create our Data Frame\n",
    "    df_tweets1.to_csv(path + filename, sep=',', index=False)\n",
    "    \n",
    "    print(f'Done with data from {season} {period}!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing hashtags in variable \n",
    "eredivisie_clubs= '#AdoDenHaag OR #AFCAjax OR #AZalkmaar OR #FCEmmen OR #FCGroningen OR #FCTwente OR #FCUtrecht OR #Feyenoord OR #FortunaSittard OR #Heracles OR #PECZwolle OR #PSV OR #RKCWaalwijk OR #SCHeerenveen OR #SpartaRotterdam OR #Vitesse OR #VVVVenlo OR #WillemII'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now scraping tweets for Eredivisie Hashtags, Season X and Period X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Season 19/20, period 1\n",
    "get_twitterdata(filename= 'season19-20period1.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2019-11-22',\n",
    "                until = '2019-11-24',\n",
    "                season= 'season19/20',\n",
    "                period= 'period1'#,Uncomment next line to test scraper for only 100 tweets!\n",
    "                #maxTweets = 100\n",
    "               )\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 19/20, period 2\n",
    "get_twitterdata(filename= 'season19-20period2.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2020-01-24',\n",
    "                until = '2020-01-26',\n",
    "                season= 'season19/20',\n",
    "                period= 'period2'#,Uncomment next line to test scraper for only 100 tweets!\n",
    "                #maxTweets = 100\n",
    "               )\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 20/21, period 1\n",
    "get_twitterdata(filename= 'season20-21period1.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2020-11-27',\n",
    "                until = '2020-11-29',\n",
    "                season= 'season20/21',\n",
    "                period= 'period1'#,Uncomment next line to test scraper for only 100 tweets!\n",
    "                #maxTweets = 100\n",
    "               )\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# Season 20/21, period 2\n",
    "get_twitterdata(filename= 'season20-21period2.csv',\n",
    "                path= \"../../oDCM/data/\",\n",
    "                searchq= eredivisie_clubs,\n",
    "                since = '2021-01-22',\n",
    "                until = '2021-01-24',\n",
    "                season= 'season20/21',\n",
    "                period= 'period2'#,Uncomment next line to test scraper for only 100 tweets!\n",
    "                #maxTweets = 100\n",
    "               )\n",
    "\n",
    "print(\"All done! :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now merging and exporting local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find_All_CSVs = ['../../oDCM/data/season19-20period1.csv',\n",
    "                 '../../oDCM/data/season19-20period2.csv',\n",
    "                 '../../oDCM/data/season20-21period1.csv',\n",
    "                 '../../oDCM/data/season20-21period2.csv']\n",
    "\n",
    "# Combine all rows of CSV-files in the list above to one 'merged_tweets_eredivisie.csv' file.\n",
    "\n",
    "combined_csv = pd.concat([pd.read_csv(f,header=0) for f in Find_All_CSVs])\n",
    "\n",
    "combined_csv.head()\n",
    "\n",
    "combined_csv.to_csv(\"../../oDCM/data/merged_tweets_eredivisie.csv\", quotechar='\"', quoting=csv.QUOTE_ALL, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"All done! Twitter data available in your directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now exporting to Dropbox (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment full codeblock below (select all, use shortcut *ctr* + */* or *cmd* + */*) and add own Dropbox-token to directly upload scraped dataset to own dropbox folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransferData:\n",
    "#     def __init__(self, access_token):\n",
    "#         self.access_token = access_token\n",
    "\n",
    "#     def upload_file(self, file_from, file_to):\n",
    "#         \"\"\"upload a file to Dropbox using API v2\n",
    "#         \"\"\"\n",
    "#         dbx = dropbox.Dropbox(self.access_token)\n",
    "\n",
    "#         with open(file_from, 'rb') as f:\n",
    "#             dbx.files_upload(f.read(), file_to)\n",
    "\n",
    "# def main():\n",
    "#     access_token = 'Dropbox-Token'\n",
    "#     transferData = TransferData(access_token)\n",
    "\n",
    "#     file_from = 'merged_tweets_eredivisie.csv'\n",
    "#     file_to = '/Apps/dPrep/merged_tweets_eredivisie.csv'  # The full path to upload the file to, including the file name\n",
    "\n",
    "#     # API v2\n",
    "#     transferData.upload_file(file_from, file_to)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "    \n",
    "# print(\"All done, check out the dropbox folder: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
