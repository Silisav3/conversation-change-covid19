# Drop observations with V1 <= -0.9
df_cleaned <- df_merged[df_merged$V1 > -0.9,]
# Load merged data
load("./gen/data-preparation/temp/dataframe_eredivisie.RData")
# Load merged data
load("./gen/data-preparation/temp/dataframe_eredivisie.RData")
df_cleaned <- dataframe_eredivisie
# Download dataset Eredivisie
# dir.create('./data/dataset1')  # Uncomment if need to create directory with R
download.file('https://www.dropbox.com/s/yklyw87gkflyfwe/merged_tweets_eredivisie.csv?dl=1','././data/dataset_eredivisie.csv')
load("C:/Users/stan/team_project/conversation-change-covid19/gen/data-preparation/output/data_cleaned.RData")
View(df_cleaned)
View(df_cleaned)
View(df_cleaned)
# Load datasets into R
df <- read.csv("./gen/data-preparation/input/dataset_eredivisie.csv")
# Save dataframe data
save(df,file="./gen/data-preparation/temp/dataframe_eredivisie.RData")
# change Tweet Text encoding to utf-8 and store in df_cleaned
df_cleaned <- iconv(df$Text, to="utf-8")
df <- read.csv(file.choose())
df <- read.csv(file.choose())
df_cleaned <- df
df_cleaned <- iconv(df$Text, to="utf-8")
# change to corpus file (needed to do analysis)
corpus <- Corpus(VectorSource(df_cleaned))
# loading library for natural language processing (NLP, text analysis) - only when not installed
if(!require(tm)){
install.packages("tm")
library(tm)
}
# change Tweet Text encoding to utf-8 and store in df_cleaned
df_cleaned <- iconv(df$Text, to="utf-8")
# change to corpus file (needed to do analysis)
corpus <- Corpus(VectorSource(df_cleaned))
View(corpus)
# Inspecting corpus file
inspect(corpus[1:5])
# clean text for data analysis
# first: lowercase everything
corpus <- tm_map(corpus, tolower)
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove some common words
corpus <- tm_map(corpus, removeWords, stopwords('dutch'))
# function to remove url's from text
remove_urls <- function(x) gsub('http[[:alnum:]]*', '', x)
# actually removing url's form text
corpus <- tm_map(corpus, content_transformer(remove_urls))
# remove unnecessary blank spaced
corpus <- tm_map(corpus, stripWhitespace)
clean_corp <- corpus
# unstructured text data to structured data by using term document matrix
tdm <- TermDocumentMatrix(clean_corp)
tdm <- as.matrix(tdm)
# unstructured text data to structured data by using term document matrix
tdm <- TermDocumentMatrix(clean_corp)
tdm <- as.matrix(tdm)
df_cleaned <- tdm
View(df_cleaned)
View(df_cleaned)
View(df_cleaned)
View(df_cleaned)
View(df_cleaned)
df_cleaned <- tdm
# loading library for natural language processing (NLP, text analysis) - only when not installed
if(!require(tm)){
install.packages("tm")
library(tm)
}
#df <- read.csv(file.choose())
input_corpus <- df
# change Tweet Text encoding to utf-8 and store in input_corpus
input_corpus <- iconv(df$Text, to="utf-8")
# change to corpus file (needed to do analysis)
corpus <- Corpus(VectorSource(input_corpus))
# clean text for data analysis
# first: lowercase everything
corpus <- tm_map(corpus, tolower)
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove some common words
corpus <- tm_map(corpus, removeWords, stopwords('dutch'))
# function to remove url's from text
remove_urls <- function(x) gsub('http[[:alnum:]]*', '', x)
# actually removing url's form text
corpus <- tm_map(corpus, content_transformer(remove_urls))
# remove unnecessary blank spaced
corpus <- tm_map(corpus, stripWhitespace)
clean_corp <- corpus
# unstructured text data to structured data by using term document matrix
tdm <- TermDocumentMatrix(clean_corp)
tdm <- as.matrix(tdm)
write.csv("././gen/data-preparation/temp/tdm.csv")
# unstructured text data to structured data by using term document matrix
tdm <- TermDocumentMatrix(clean_corp)
tdm <- as.matrix(tdm)
write.csv(tdm, "././gen/data-preparation/temp/tdm.csv")
df_cleaned <- df
# change Tweet Text encoding to utf-8 and store in df_cleaned
df_cleaned <- iconv(df$Text, to="utf-8")
# Save cleaned data
save(df_cleaned,file="./gen/data-preparation/output/data_cleaned.RData")
# loading data for sentiment analysis, storing only the tweets (text) in 'sentiment' variable
sentiment_pre <- df %>%
filter(Season=="season19/20")
sentiment_post <- df %>%
filter(Season=="season20/21")
sentiment_pre <- iconv(sentiment_pre$Text, to="utf-8")
sentiment_post <- iconv(sentiment_post$Text, to="utf-8")
sentiment_pre <- df %>%
filter(Season=="season19/20")
# ------- Sentiment analysis --------#
# installing packages
install.packages('syuzhet')
install.packages("lubridate")
install.packages('scales')
install.packages('reshape2')
# loading packages
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# loading data for sentiment analysis, storing only the tweets (text) in 'sentiment' variable
sentiment_pre <- df %>%
filter(Season=="season19/20")
sentiment_post <- df %>%
filter(Season=="season20/21")
sentiment_pre <- iconv(sentiment_pre$Text, to="utf-8")
sentiment_post <- iconv(sentiment_post$Text, to="utf-8")
# loading data for sentiment analysis, storing only the tweets (text) in 'sentiment' variable
sentiment_pre <- df %>%
filter(Season=="season19/20")
sentiment_post <- df %>%
filter(Season=="season20/21")
sentiment_pre <- iconv(sentiment_pre$Text, to="utf-8")
sentiment_post <- iconv(sentiment_post$Text, to="utf-8")
# obtain sentiment scores - store in dataframe: each row is a tweet.
# scores per tweet for:
# anger, anticipation, disgust, fear, joy, sadness, surprise, trust, negative & positive.
scores_pre <- get_nrc_sentiment(sentiment_pre, language = 'dutch')
scores_post <- get_nrc_sentiment(sentiment_post, language = 'dutch')
# Save cleaned data
save(scores_pre,file="./gen/data-preparation/output/scores_pre.RData")
save(scores_post,file="./gen/data-preparation/output/scores_post.RData")
# installing packages
install.packages("tm")
install.packages('syuzhet')
install.packages("lubridate")
install.packages('scales')
install.packages('reshape2')
# loading packages
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(tm)
install.packages("progress")
View(scores_pre)
